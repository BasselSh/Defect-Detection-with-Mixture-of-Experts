{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmengine\n",
    "import mmcv\n",
    "from mmcv.transforms import Compose, Resize\n",
    "import matplotlib.pyplot as plt\n",
    "from mmdet.utils import get_test_pipeline_cfg\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as torch_trans\n",
    "from mmdet.apis import init_detector, inference_detector \n",
    "from wrappers import *\n",
    "from mmdet.visualization import DetLocalVisualizer\n",
    "from mmengine.registry import MODELS\n",
    "from mmengine.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: weights/swin_tiny_patch4_window7_224.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: model\n",
      "\n",
      "missing keys in source state_dict: backbone.patch_embed.projection.weight, backbone.patch_embed.projection.bias, backbone.patch_embed.norm.weight, backbone.patch_embed.norm.bias, backbone.stages.0.blocks.0.norm1.weight, backbone.stages.0.blocks.0.norm1.bias, backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.0.blocks.0.attn.w_msa.relative_position_index, backbone.stages.0.blocks.0.attn.w_msa.qkv.weight, backbone.stages.0.blocks.0.attn.w_msa.qkv.bias, backbone.stages.0.blocks.0.attn.w_msa.proj.weight, backbone.stages.0.blocks.0.attn.w_msa.proj.bias, backbone.stages.0.blocks.0.norm2.weight, backbone.stages.0.blocks.0.norm2.bias, backbone.stages.0.blocks.0.ffn.layers.0.0.weight, backbone.stages.0.blocks.0.ffn.layers.0.0.bias, backbone.stages.0.blocks.0.ffn.layers.1.weight, backbone.stages.0.blocks.0.ffn.layers.1.bias, backbone.stages.0.blocks.1.norm1.weight, backbone.stages.0.blocks.1.norm1.bias, backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.0.blocks.1.attn.w_msa.relative_position_index, backbone.stages.0.blocks.1.attn.w_msa.qkv.weight, backbone.stages.0.blocks.1.attn.w_msa.qkv.bias, backbone.stages.0.blocks.1.attn.w_msa.proj.weight, backbone.stages.0.blocks.1.attn.w_msa.proj.bias, backbone.stages.0.blocks.1.norm2.weight, backbone.stages.0.blocks.1.norm2.bias, backbone.stages.0.blocks.1.ffn.layers.0.0.weight, backbone.stages.0.blocks.1.ffn.layers.0.0.bias, backbone.stages.0.blocks.1.ffn.layers.1.weight, backbone.stages.0.blocks.1.ffn.layers.1.bias, backbone.stages.0.downsample.norm.weight, backbone.stages.0.downsample.norm.bias, backbone.stages.0.downsample.reduction.weight, backbone.stages.1.blocks.0.norm1.weight, backbone.stages.1.blocks.0.norm1.bias, backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.1.blocks.0.attn.w_msa.relative_position_index, backbone.stages.1.blocks.0.attn.w_msa.qkv.weight, backbone.stages.1.blocks.0.attn.w_msa.qkv.bias, backbone.stages.1.blocks.0.attn.w_msa.proj.weight, backbone.stages.1.blocks.0.attn.w_msa.proj.bias, backbone.stages.1.blocks.0.norm2.weight, backbone.stages.1.blocks.0.norm2.bias, backbone.stages.1.blocks.0.ffn.layers.0.0.weight, backbone.stages.1.blocks.0.ffn.layers.0.0.bias, backbone.stages.1.blocks.0.ffn.layers.1.weight, backbone.stages.1.blocks.0.ffn.layers.1.bias, backbone.stages.1.blocks.1.norm1.weight, backbone.stages.1.blocks.1.norm1.bias, backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.1.blocks.1.attn.w_msa.relative_position_index, backbone.stages.1.blocks.1.attn.w_msa.qkv.weight, backbone.stages.1.blocks.1.attn.w_msa.qkv.bias, backbone.stages.1.blocks.1.attn.w_msa.proj.weight, backbone.stages.1.blocks.1.attn.w_msa.proj.bias, backbone.stages.1.blocks.1.norm2.weight, backbone.stages.1.blocks.1.norm2.bias, backbone.stages.1.blocks.1.ffn.layers.0.0.weight, backbone.stages.1.blocks.1.ffn.layers.0.0.bias, backbone.stages.1.blocks.1.ffn.layers.1.weight, backbone.stages.1.blocks.1.ffn.layers.1.bias, backbone.stages.1.downsample.norm.weight, backbone.stages.1.downsample.norm.bias, backbone.stages.1.downsample.reduction.weight, backbone.stages.2.blocks.0.norm1.weight, backbone.stages.2.blocks.0.norm1.bias, backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.0.attn.w_msa.relative_position_index, backbone.stages.2.blocks.0.attn.w_msa.qkv.weight, backbone.stages.2.blocks.0.attn.w_msa.qkv.bias, backbone.stages.2.blocks.0.attn.w_msa.proj.weight, backbone.stages.2.blocks.0.attn.w_msa.proj.bias, backbone.stages.2.blocks.0.norm2.weight, backbone.stages.2.blocks.0.norm2.bias, backbone.stages.2.blocks.0.ffn.layers.0.0.weight, backbone.stages.2.blocks.0.ffn.layers.0.0.bias, backbone.stages.2.blocks.0.ffn.layers.1.weight, backbone.stages.2.blocks.0.ffn.layers.1.bias, backbone.stages.2.blocks.1.norm1.weight, backbone.stages.2.blocks.1.norm1.bias, backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.1.attn.w_msa.qkv.weight, backbone.stages.2.blocks.1.attn.w_msa.qkv.bias, backbone.stages.2.blocks.1.attn.w_msa.proj.weight, backbone.stages.2.blocks.1.attn.w_msa.proj.bias, backbone.stages.2.blocks.1.norm2.weight, backbone.stages.2.blocks.1.norm2.bias, backbone.stages.2.blocks.1.ffn.layers.0.0.weight, backbone.stages.2.blocks.1.ffn.layers.0.0.bias, backbone.stages.2.blocks.1.ffn.layers.1.weight, backbone.stages.2.blocks.1.ffn.layers.1.bias, backbone.stages.2.blocks.2.norm1.weight, backbone.stages.2.blocks.2.norm1.bias, backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.2.attn.w_msa.relative_position_index, backbone.stages.2.blocks.2.attn.w_msa.qkv.weight, backbone.stages.2.blocks.2.attn.w_msa.qkv.bias, backbone.stages.2.blocks.2.attn.w_msa.proj.weight, backbone.stages.2.blocks.2.attn.w_msa.proj.bias, backbone.stages.2.blocks.2.norm2.weight, backbone.stages.2.blocks.2.norm2.bias, backbone.stages.2.blocks.2.ffn.layers.0.0.weight, backbone.stages.2.blocks.2.ffn.layers.0.0.bias, backbone.stages.2.blocks.2.ffn.layers.1.weight, backbone.stages.2.blocks.2.ffn.layers.1.bias, backbone.stages.2.blocks.3.norm1.weight, backbone.stages.2.blocks.3.norm1.bias, backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.3.attn.w_msa.relative_position_index, backbone.stages.2.blocks.3.attn.w_msa.qkv.weight, backbone.stages.2.blocks.3.attn.w_msa.qkv.bias, backbone.stages.2.blocks.3.attn.w_msa.proj.weight, backbone.stages.2.blocks.3.attn.w_msa.proj.bias, backbone.stages.2.blocks.3.norm2.weight, backbone.stages.2.blocks.3.norm2.bias, backbone.stages.2.blocks.3.ffn.layers.0.0.weight, backbone.stages.2.blocks.3.ffn.layers.0.0.bias, backbone.stages.2.blocks.3.ffn.layers.1.weight, backbone.stages.2.blocks.3.ffn.layers.1.bias, backbone.stages.2.blocks.4.norm1.weight, backbone.stages.2.blocks.4.norm1.bias, backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.4.attn.w_msa.relative_position_index, backbone.stages.2.blocks.4.attn.w_msa.qkv.weight, backbone.stages.2.blocks.4.attn.w_msa.qkv.bias, backbone.stages.2.blocks.4.attn.w_msa.proj.weight, backbone.stages.2.blocks.4.attn.w_msa.proj.bias, backbone.stages.2.blocks.4.norm2.weight, backbone.stages.2.blocks.4.norm2.bias, backbone.stages.2.blocks.4.ffn.layers.0.0.weight, backbone.stages.2.blocks.4.ffn.layers.0.0.bias, backbone.stages.2.blocks.4.ffn.layers.1.weight, backbone.stages.2.blocks.4.ffn.layers.1.bias, backbone.stages.2.blocks.5.norm1.weight, backbone.stages.2.blocks.5.norm1.bias, backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.5.attn.w_msa.relative_position_index, backbone.stages.2.blocks.5.attn.w_msa.qkv.weight, backbone.stages.2.blocks.5.attn.w_msa.qkv.bias, backbone.stages.2.blocks.5.attn.w_msa.proj.weight, backbone.stages.2.blocks.5.attn.w_msa.proj.bias, backbone.stages.2.blocks.5.norm2.weight, backbone.stages.2.blocks.5.norm2.bias, backbone.stages.2.blocks.5.ffn.layers.0.0.weight, backbone.stages.2.blocks.5.ffn.layers.0.0.bias, backbone.stages.2.blocks.5.ffn.layers.1.weight, backbone.stages.2.blocks.5.ffn.layers.1.bias, backbone.stages.2.downsample.norm.weight, backbone.stages.2.downsample.norm.bias, backbone.stages.2.downsample.reduction.weight, backbone.stages.3.blocks.0.norm1.weight, backbone.stages.3.blocks.0.norm1.bias, backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.3.blocks.0.attn.w_msa.relative_position_index, backbone.stages.3.blocks.0.attn.w_msa.qkv.weight, backbone.stages.3.blocks.0.attn.w_msa.qkv.bias, backbone.stages.3.blocks.0.attn.w_msa.proj.weight, backbone.stages.3.blocks.0.attn.w_msa.proj.bias, backbone.stages.3.blocks.0.norm2.weight, backbone.stages.3.blocks.0.norm2.bias, backbone.stages.3.blocks.0.ffn.layers.0.0.weight, backbone.stages.3.blocks.0.ffn.layers.0.0.bias, backbone.stages.3.blocks.0.ffn.layers.1.weight, backbone.stages.3.blocks.0.ffn.layers.1.bias, backbone.stages.3.blocks.1.norm1.weight, backbone.stages.3.blocks.1.norm1.bias, backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.3.blocks.1.attn.w_msa.relative_position_index, backbone.stages.3.blocks.1.attn.w_msa.qkv.weight, backbone.stages.3.blocks.1.attn.w_msa.qkv.bias, backbone.stages.3.blocks.1.attn.w_msa.proj.weight, backbone.stages.3.blocks.1.attn.w_msa.proj.bias, backbone.stages.3.blocks.1.norm2.weight, backbone.stages.3.blocks.1.norm2.bias, backbone.stages.3.blocks.1.ffn.layers.0.0.weight, backbone.stages.3.blocks.1.ffn.layers.0.0.bias, backbone.stages.3.blocks.1.ffn.layers.1.weight, backbone.stages.3.blocks.1.ffn.layers.1.bias, backbone.norm0.weight, backbone.norm0.bias, backbone.norm1.weight, backbone.norm1.bias, backbone.norm2.weight, backbone.norm2.bias, backbone.norm3.weight, backbone.norm3.bias, neck.lateral_convs.0.conv.weight, neck.lateral_convs.0.conv.bias, neck.lateral_convs.1.conv.weight, neck.lateral_convs.1.conv.bias, neck.lateral_convs.2.conv.weight, neck.lateral_convs.2.conv.bias, neck.lateral_convs.3.conv.weight, neck.lateral_convs.3.conv.bias, neck.fpn_convs.0.conv.weight, neck.fpn_convs.0.conv.bias, neck.fpn_convs.1.conv.weight, neck.fpn_convs.1.conv.bias, neck.fpn_convs.2.conv.weight, neck.fpn_convs.2.conv.bias, neck.fpn_convs.3.conv.weight, neck.fpn_convs.3.conv.bias, rpn_head.rpn_conv.weight, rpn_head.rpn_conv.bias, rpn_head.rpn_cls.weight, rpn_head.rpn_cls.bias, rpn_head.rpn_reg.weight, rpn_head.rpn_reg.bias, roi_head.bbox_head.fc_cls.weight, roi_head.bbox_head.fc_cls.bias, roi_head.bbox_head.fc_reg.weight, roi_head.bbox_head.fc_reg.bias, roi_head.bbox_head.shared_fcs.0.weight, roi_head.bbox_head.shared_fcs.0.bias, roi_head.bbox_head.shared_fcs.1.weight, roi_head.bbox_head.shared_fcs.1.bias\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huemorgen/miniconda3/envs/mmlab/lib/python3.8/site-packages/mmdet/apis/inference.py:90: UserWarning: dataset_meta or class names are not saved in the checkpoint's meta data, use COCO classes by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = \"configs/swin/swin_tiny.py\"\n",
    "# model_path = \"configs/swin_based/swin_tiny.py\"\n",
    "ckpt_path = \"weights/swin_tiny_patch4_window7_224.pth\"\n",
    "cfg = Config.fromfile(model_path)\n",
    "\n",
    "model = init_detector(cfg, ckpt_path)\n",
    "# model = MODELS.build(cfg.model)\n",
    "\n",
    "vis = DetLocalVisualizer()\n",
    "palette = [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (0, 200, 30), (100, 200, 150), (100, 200, 50), (10, 200, 50)]\n",
    "CLASSES = ('1','2','3','4','5','6')\n",
    "img = mmcv.imread(\"data/mmdet/NEU_DET/test/inclusion_243.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/01 16:42:30 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "06/01 16:42:30 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n"
     ]
    }
   ],
   "source": [
    "from mmengine.runner.checkpoint import save_checkpoint\n",
    "file = 'swin_base_model.pth'\n",
    "save_checkpoint(model.backbone.state_dict(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): SwinBlockSequence(\n",
      "    (blocks): ModuleList(\n",
      "      (0-1): 2 x SwinBlock(\n",
      "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftWindowMSA(\n",
      "          (w_msa): WindowMSA(\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop): DropPath()\n",
      "        )\n",
      "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "          (gamma2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (downsample): PatchMerging(\n",
      "      (adap_padding): AdaptivePadding()\n",
      "      (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))\n",
      "      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (1): SwinBlockSequence(\n",
      "    (blocks): ModuleList(\n",
      "      (0-1): 2 x SwinBlock(\n",
      "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftWindowMSA(\n",
      "          (w_msa): WindowMSA(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop): DropPath()\n",
      "        )\n",
      "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "          (gamma2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (downsample): PatchMerging(\n",
      "      (adap_padding): AdaptivePadding()\n",
      "      (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (2): SwinBlockSequence(\n",
      "    (blocks): ModuleList(\n",
      "      (0-5): 6 x SwinBlock(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftWindowMSA(\n",
      "          (w_msa): WindowMSA(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop): DropPath()\n",
      "        )\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "          (gamma2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (downsample): PatchMerging(\n",
      "      (adap_padding): AdaptivePadding()\n",
      "      (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))\n",
      "      (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (3): SwinBlockSequenceMOE(\n",
      "    (blocks): ModuleList(\n",
      "      (0): SwinBlock(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftWindowMSA(\n",
      "          (w_msa): WindowMSA(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop): DropPath()\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): SwitchMoEFFN(\n",
      "          (experts): ModuleList(\n",
      "            (0-1): 2 x FFN(\n",
      "              (layers): Sequential(\n",
      "                (0): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (1): ReLU(inplace=True)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (1): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): Identity()\n",
      "              (gamma2): Identity()\n",
      "            )\n",
      "          )\n",
      "          (gate): SwitchGate(\n",
      "            (w_gate): Linear(in_features=768, out_features=2, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): SwinBlock(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ShiftWindowMSA(\n",
      "          (w_msa): WindowMSA(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop): DropPath()\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "          (gamma2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huemorgen/miniconda3/envs/mmlab/lib/python3.8/site-packages/mmdet/apis/inference.py:70: UserWarning: checkpoint is None, use COCO classes by default.\n",
      "  warnings.warn('checkpoint is None, use COCO classes by default.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = \"configs/swin/swin_moe.py\"\n",
    "cfg = Config.fromfile(model_path)\n",
    "model = init_detector(cfg)\n",
    "backbone = model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: swin_base_model.pth\n"
     ]
    }
   ],
   "source": [
    "from mmengine.runner.checkpoint import _load_checkpoint\n",
    "ckpt_path = file\n",
    "checkpoint = _load_checkpoint(ckpt_path, 'cpu')\n",
    "ckp_state_dict = checkpoint\n",
    "backbone_state_dict = backbone.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gates = []\n",
    "for i, key in enumerate(backbone_state_dict.keys()):\n",
    "    if 'gate' in key:\n",
    "        gates.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stages.3.blocks.0.ffn.gate.w_gate.weight',\n",
       " 'stages.3.blocks.0.ffn.gate.w_gate.bias']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experts = []\n",
    "for i, key in enumerate(backbone_state_dict.keys()):\n",
    "    if 'experts' in key:\n",
    "        experts.append(key)\n",
    "\n",
    "ffn_ckpt = []\n",
    "for i, key in enumerate(ckp_state_dict.keys()):\n",
    "    if 'ffn' in key:\n",
    "        ffn_ckpt.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['stages.3.blocks.0.ffn.experts.0.layers.0.0.weight',\n",
       "  'stages.3.blocks.0.ffn.experts.0.layers.0.0.bias',\n",
       "  'stages.3.blocks.0.ffn.experts.0.layers.1.weight',\n",
       "  'stages.3.blocks.0.ffn.experts.0.layers.1.bias',\n",
       "  'stages.3.blocks.0.ffn.experts.1.layers.0.0.weight',\n",
       "  'stages.3.blocks.0.ffn.experts.1.layers.0.0.bias',\n",
       "  'stages.3.blocks.0.ffn.experts.1.layers.1.weight',\n",
       "  'stages.3.blocks.0.ffn.experts.1.layers.1.bias'],\n",
       " ['stages.0.blocks.0.ffn.layers.0.0.weight',\n",
       "  'stages.0.blocks.0.ffn.layers.0.0.bias',\n",
       "  'stages.0.blocks.0.ffn.layers.1.weight',\n",
       "  'stages.0.blocks.0.ffn.layers.1.bias',\n",
       "  'stages.0.blocks.1.ffn.layers.0.0.weight',\n",
       "  'stages.0.blocks.1.ffn.layers.0.0.bias',\n",
       "  'stages.0.blocks.1.ffn.layers.1.weight',\n",
       "  'stages.0.blocks.1.ffn.layers.1.bias',\n",
       "  'stages.1.blocks.0.ffn.layers.0.0.weight',\n",
       "  'stages.1.blocks.0.ffn.layers.0.0.bias',\n",
       "  'stages.1.blocks.0.ffn.layers.1.weight',\n",
       "  'stages.1.blocks.0.ffn.layers.1.bias',\n",
       "  'stages.1.blocks.1.ffn.layers.0.0.weight',\n",
       "  'stages.1.blocks.1.ffn.layers.0.0.bias',\n",
       "  'stages.1.blocks.1.ffn.layers.1.weight',\n",
       "  'stages.1.blocks.1.ffn.layers.1.bias',\n",
       "  'stages.2.blocks.0.ffn.layers.0.0.weight',\n",
       "  'stages.2.blocks.0.ffn.layers.0.0.bias',\n",
       "  'stages.2.blocks.0.ffn.layers.1.weight',\n",
       "  'stages.2.blocks.0.ffn.layers.1.bias',\n",
       "  'stages.2.blocks.1.ffn.layers.0.0.weight',\n",
       "  'stages.2.blocks.1.ffn.layers.0.0.bias',\n",
       "  'stages.2.blocks.1.ffn.layers.1.weight',\n",
       "  'stages.2.blocks.1.ffn.layers.1.bias',\n",
       "  'stages.2.blocks.2.ffn.layers.0.0.weight',\n",
       "  'stages.2.blocks.2.ffn.layers.0.0.bias',\n",
       "  'stages.2.blocks.2.ffn.layers.1.weight',\n",
       "  'stages.2.blocks.2.ffn.layers.1.bias',\n",
       "  'stages.2.blocks.3.ffn.layers.0.0.weight',\n",
       "  'stages.2.blocks.3.ffn.layers.0.0.bias',\n",
       "  'stages.2.blocks.3.ffn.layers.1.weight',\n",
       "  'stages.2.blocks.3.ffn.layers.1.bias',\n",
       "  'stages.2.blocks.4.ffn.layers.0.0.weight',\n",
       "  'stages.2.blocks.4.ffn.layers.0.0.bias',\n",
       "  'stages.2.blocks.4.ffn.layers.1.weight',\n",
       "  'stages.2.blocks.4.ffn.layers.1.bias',\n",
       "  'stages.2.blocks.5.ffn.layers.0.0.weight',\n",
       "  'stages.2.blocks.5.ffn.layers.0.0.bias',\n",
       "  'stages.2.blocks.5.ffn.layers.1.weight',\n",
       "  'stages.2.blocks.5.ffn.layers.1.bias',\n",
       "  'stages.3.blocks.0.ffn.layers.0.0.weight',\n",
       "  'stages.3.blocks.0.ffn.layers.0.0.bias',\n",
       "  'stages.3.blocks.0.ffn.layers.1.weight',\n",
       "  'stages.3.blocks.0.ffn.layers.1.bias',\n",
       "  'stages.3.blocks.1.ffn.layers.0.0.weight',\n",
       "  'stages.3.blocks.1.ffn.layers.0.0.bias',\n",
       "  'stages.3.blocks.1.ffn.layers.1.weight',\n",
       "  'stages.3.blocks.1.ffn.layers.1.bias'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experts,ffn_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module\n",
      "patch_embed.projection.weight\n",
      "patch_embed.projection.weight\n",
      "Module\n",
      "patch_embed.projection.bias\n",
      "patch_embed.projection.bias\n",
      "Module\n",
      "patch_embed.norm.weight\n",
      "patch_embed.norm.weight\n",
      "Module\n",
      "patch_embed.norm.bias\n",
      "patch_embed.norm.bias\n",
      "Module\n",
      "stages.0.blocks.0.norm1.weight\n",
      "stages.0.blocks.0.norm1.weight\n",
      "Module\n",
      "stages.0.blocks.0.norm1.bias\n",
      "stages.0.blocks.0.norm1.bias\n",
      "Module\n",
      "stages.0.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "stages.0.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.0.blocks.0.attn.w_msa.relative_position_index\n",
      "stages.0.blocks.0.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.0.blocks.0.attn.w_msa.qkv.weight\n",
      "stages.0.blocks.0.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.0.blocks.0.attn.w_msa.qkv.bias\n",
      "stages.0.blocks.0.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.0.blocks.0.attn.w_msa.proj.weight\n",
      "stages.0.blocks.0.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.0.blocks.0.attn.w_msa.proj.bias\n",
      "stages.0.blocks.0.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.0.blocks.0.norm2.weight\n",
      "stages.0.blocks.0.norm2.weight\n",
      "Module\n",
      "stages.0.blocks.0.norm2.bias\n",
      "stages.0.blocks.0.norm2.bias\n",
      "Module\n",
      "stages.0.blocks.0.ffn.layers.0.0.weight\n",
      "stages.0.blocks.0.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.0.blocks.0.ffn.layers.0.0.bias\n",
      "stages.0.blocks.0.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.0.blocks.0.ffn.layers.1.weight\n",
      "stages.0.blocks.0.ffn.layers.1.weight\n",
      "Module\n",
      "stages.0.blocks.0.ffn.layers.1.bias\n",
      "stages.0.blocks.0.ffn.layers.1.bias\n",
      "Module\n",
      "stages.0.blocks.1.norm1.weight\n",
      "stages.0.blocks.1.norm1.weight\n",
      "Module\n",
      "stages.0.blocks.1.norm1.bias\n",
      "stages.0.blocks.1.norm1.bias\n",
      "Module\n",
      "stages.0.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "stages.0.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.0.blocks.1.attn.w_msa.relative_position_index\n",
      "stages.0.blocks.1.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.0.blocks.1.attn.w_msa.qkv.weight\n",
      "stages.0.blocks.1.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.0.blocks.1.attn.w_msa.qkv.bias\n",
      "stages.0.blocks.1.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.0.blocks.1.attn.w_msa.proj.weight\n",
      "stages.0.blocks.1.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.0.blocks.1.attn.w_msa.proj.bias\n",
      "stages.0.blocks.1.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.0.blocks.1.norm2.weight\n",
      "stages.0.blocks.1.norm2.weight\n",
      "Module\n",
      "stages.0.blocks.1.norm2.bias\n",
      "stages.0.blocks.1.norm2.bias\n",
      "Module\n",
      "stages.0.blocks.1.ffn.layers.0.0.weight\n",
      "stages.0.blocks.1.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.0.blocks.1.ffn.layers.0.0.bias\n",
      "stages.0.blocks.1.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.0.blocks.1.ffn.layers.1.weight\n",
      "stages.0.blocks.1.ffn.layers.1.weight\n",
      "Module\n",
      "stages.0.blocks.1.ffn.layers.1.bias\n",
      "stages.0.blocks.1.ffn.layers.1.bias\n",
      "Module\n",
      "stages.0.downsample.norm.weight\n",
      "stages.0.downsample.norm.weight\n",
      "Module\n",
      "stages.0.downsample.norm.bias\n",
      "stages.0.downsample.norm.bias\n",
      "Module\n",
      "stages.0.downsample.reduction.weight\n",
      "stages.0.downsample.reduction.weight\n",
      "Module\n",
      "stages.1.blocks.0.norm1.weight\n",
      "stages.1.blocks.0.norm1.weight\n",
      "Module\n",
      "stages.1.blocks.0.norm1.bias\n",
      "stages.1.blocks.0.norm1.bias\n",
      "Module\n",
      "stages.1.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "stages.1.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.1.blocks.0.attn.w_msa.relative_position_index\n",
      "stages.1.blocks.0.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.1.blocks.0.attn.w_msa.qkv.weight\n",
      "stages.1.blocks.0.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.1.blocks.0.attn.w_msa.qkv.bias\n",
      "stages.1.blocks.0.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.1.blocks.0.attn.w_msa.proj.weight\n",
      "stages.1.blocks.0.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.1.blocks.0.attn.w_msa.proj.bias\n",
      "stages.1.blocks.0.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.1.blocks.0.norm2.weight\n",
      "stages.1.blocks.0.norm2.weight\n",
      "Module\n",
      "stages.1.blocks.0.norm2.bias\n",
      "stages.1.blocks.0.norm2.bias\n",
      "Module\n",
      "stages.1.blocks.0.ffn.layers.0.0.weight\n",
      "stages.1.blocks.0.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.1.blocks.0.ffn.layers.0.0.bias\n",
      "stages.1.blocks.0.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.1.blocks.0.ffn.layers.1.weight\n",
      "stages.1.blocks.0.ffn.layers.1.weight\n",
      "Module\n",
      "stages.1.blocks.0.ffn.layers.1.bias\n",
      "stages.1.blocks.0.ffn.layers.1.bias\n",
      "Module\n",
      "stages.1.blocks.1.norm1.weight\n",
      "stages.1.blocks.1.norm1.weight\n",
      "Module\n",
      "stages.1.blocks.1.norm1.bias\n",
      "stages.1.blocks.1.norm1.bias\n",
      "Module\n",
      "stages.1.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "stages.1.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.1.blocks.1.attn.w_msa.relative_position_index\n",
      "stages.1.blocks.1.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.1.blocks.1.attn.w_msa.qkv.weight\n",
      "stages.1.blocks.1.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.1.blocks.1.attn.w_msa.qkv.bias\n",
      "stages.1.blocks.1.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.1.blocks.1.attn.w_msa.proj.weight\n",
      "stages.1.blocks.1.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.1.blocks.1.attn.w_msa.proj.bias\n",
      "stages.1.blocks.1.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.1.blocks.1.norm2.weight\n",
      "stages.1.blocks.1.norm2.weight\n",
      "Module\n",
      "stages.1.blocks.1.norm2.bias\n",
      "stages.1.blocks.1.norm2.bias\n",
      "Module\n",
      "stages.1.blocks.1.ffn.layers.0.0.weight\n",
      "stages.1.blocks.1.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.1.blocks.1.ffn.layers.0.0.bias\n",
      "stages.1.blocks.1.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.1.blocks.1.ffn.layers.1.weight\n",
      "stages.1.blocks.1.ffn.layers.1.weight\n",
      "Module\n",
      "stages.1.blocks.1.ffn.layers.1.bias\n",
      "stages.1.blocks.1.ffn.layers.1.bias\n",
      "Module\n",
      "stages.1.downsample.norm.weight\n",
      "stages.1.downsample.norm.weight\n",
      "Module\n",
      "stages.1.downsample.norm.bias\n",
      "stages.1.downsample.norm.bias\n",
      "Module\n",
      "stages.1.downsample.reduction.weight\n",
      "stages.1.downsample.reduction.weight\n",
      "Module\n",
      "stages.2.blocks.0.norm1.weight\n",
      "stages.2.blocks.0.norm1.weight\n",
      "Module\n",
      "stages.2.blocks.0.norm1.bias\n",
      "stages.2.blocks.0.norm1.bias\n",
      "Module\n",
      "stages.2.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.2.blocks.0.attn.w_msa.relative_position_index\n",
      "stages.2.blocks.0.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.2.blocks.0.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.0.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.2.blocks.0.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.0.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.2.blocks.0.attn.w_msa.proj.weight\n",
      "stages.2.blocks.0.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.2.blocks.0.attn.w_msa.proj.bias\n",
      "stages.2.blocks.0.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.2.blocks.0.norm2.weight\n",
      "stages.2.blocks.0.norm2.weight\n",
      "Module\n",
      "stages.2.blocks.0.norm2.bias\n",
      "stages.2.blocks.0.norm2.bias\n",
      "Module\n",
      "stages.2.blocks.0.ffn.layers.0.0.weight\n",
      "stages.2.blocks.0.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.2.blocks.0.ffn.layers.0.0.bias\n",
      "stages.2.blocks.0.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.2.blocks.0.ffn.layers.1.weight\n",
      "stages.2.blocks.0.ffn.layers.1.weight\n",
      "Module\n",
      "stages.2.blocks.0.ffn.layers.1.bias\n",
      "stages.2.blocks.0.ffn.layers.1.bias\n",
      "Module\n",
      "stages.2.blocks.1.norm1.weight\n",
      "stages.2.blocks.1.norm1.weight\n",
      "Module\n",
      "stages.2.blocks.1.norm1.bias\n",
      "stages.2.blocks.1.norm1.bias\n",
      "Module\n",
      "stages.2.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.2.blocks.1.attn.w_msa.relative_position_index\n",
      "stages.2.blocks.1.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.2.blocks.1.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.1.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.2.blocks.1.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.1.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.2.blocks.1.attn.w_msa.proj.weight\n",
      "stages.2.blocks.1.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.2.blocks.1.attn.w_msa.proj.bias\n",
      "stages.2.blocks.1.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.2.blocks.1.norm2.weight\n",
      "stages.2.blocks.1.norm2.weight\n",
      "Module\n",
      "stages.2.blocks.1.norm2.bias\n",
      "stages.2.blocks.1.norm2.bias\n",
      "Module\n",
      "stages.2.blocks.1.ffn.layers.0.0.weight\n",
      "stages.2.blocks.1.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.2.blocks.1.ffn.layers.0.0.bias\n",
      "stages.2.blocks.1.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.2.blocks.1.ffn.layers.1.weight\n",
      "stages.2.blocks.1.ffn.layers.1.weight\n",
      "Module\n",
      "stages.2.blocks.1.ffn.layers.1.bias\n",
      "stages.2.blocks.1.ffn.layers.1.bias\n",
      "Module\n",
      "stages.2.blocks.2.norm1.weight\n",
      "stages.2.blocks.2.norm1.weight\n",
      "Module\n",
      "stages.2.blocks.2.norm1.bias\n",
      "stages.2.blocks.2.norm1.bias\n",
      "Module\n",
      "stages.2.blocks.2.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.2.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.2.blocks.2.attn.w_msa.relative_position_index\n",
      "stages.2.blocks.2.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.2.blocks.2.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.2.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.2.blocks.2.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.2.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.2.blocks.2.attn.w_msa.proj.weight\n",
      "stages.2.blocks.2.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.2.blocks.2.attn.w_msa.proj.bias\n",
      "stages.2.blocks.2.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.2.blocks.2.norm2.weight\n",
      "stages.2.blocks.2.norm2.weight\n",
      "Module\n",
      "stages.2.blocks.2.norm2.bias\n",
      "stages.2.blocks.2.norm2.bias\n",
      "Module\n",
      "stages.2.blocks.2.ffn.layers.0.0.weight\n",
      "stages.2.blocks.2.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.2.blocks.2.ffn.layers.0.0.bias\n",
      "stages.2.blocks.2.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.2.blocks.2.ffn.layers.1.weight\n",
      "stages.2.blocks.2.ffn.layers.1.weight\n",
      "Module\n",
      "stages.2.blocks.2.ffn.layers.1.bias\n",
      "stages.2.blocks.2.ffn.layers.1.bias\n",
      "Module\n",
      "stages.2.blocks.3.norm1.weight\n",
      "stages.2.blocks.3.norm1.weight\n",
      "Module\n",
      "stages.2.blocks.3.norm1.bias\n",
      "stages.2.blocks.3.norm1.bias\n",
      "Module\n",
      "stages.2.blocks.3.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.3.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.2.blocks.3.attn.w_msa.relative_position_index\n",
      "stages.2.blocks.3.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.2.blocks.3.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.3.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.2.blocks.3.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.3.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.2.blocks.3.attn.w_msa.proj.weight\n",
      "stages.2.blocks.3.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.2.blocks.3.attn.w_msa.proj.bias\n",
      "stages.2.blocks.3.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.2.blocks.3.norm2.weight\n",
      "stages.2.blocks.3.norm2.weight\n",
      "Module\n",
      "stages.2.blocks.3.norm2.bias\n",
      "stages.2.blocks.3.norm2.bias\n",
      "Module\n",
      "stages.2.blocks.3.ffn.layers.0.0.weight\n",
      "stages.2.blocks.3.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.2.blocks.3.ffn.layers.0.0.bias\n",
      "stages.2.blocks.3.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.2.blocks.3.ffn.layers.1.weight\n",
      "stages.2.blocks.3.ffn.layers.1.weight\n",
      "Module\n",
      "stages.2.blocks.3.ffn.layers.1.bias\n",
      "stages.2.blocks.3.ffn.layers.1.bias\n",
      "Module\n",
      "stages.2.blocks.4.norm1.weight\n",
      "stages.2.blocks.4.norm1.weight\n",
      "Module\n",
      "stages.2.blocks.4.norm1.bias\n",
      "stages.2.blocks.4.norm1.bias\n",
      "Module\n",
      "stages.2.blocks.4.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.4.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.2.blocks.4.attn.w_msa.relative_position_index\n",
      "stages.2.blocks.4.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.2.blocks.4.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.4.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.2.blocks.4.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.4.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.2.blocks.4.attn.w_msa.proj.weight\n",
      "stages.2.blocks.4.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.2.blocks.4.attn.w_msa.proj.bias\n",
      "stages.2.blocks.4.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.2.blocks.4.norm2.weight\n",
      "stages.2.blocks.4.norm2.weight\n",
      "Module\n",
      "stages.2.blocks.4.norm2.bias\n",
      "stages.2.blocks.4.norm2.bias\n",
      "Module\n",
      "stages.2.blocks.4.ffn.layers.0.0.weight\n",
      "stages.2.blocks.4.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.2.blocks.4.ffn.layers.0.0.bias\n",
      "stages.2.blocks.4.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.2.blocks.4.ffn.layers.1.weight\n",
      "stages.2.blocks.4.ffn.layers.1.weight\n",
      "Module\n",
      "stages.2.blocks.4.ffn.layers.1.bias\n",
      "stages.2.blocks.4.ffn.layers.1.bias\n",
      "Module\n",
      "stages.2.blocks.5.norm1.weight\n",
      "stages.2.blocks.5.norm1.weight\n",
      "Module\n",
      "stages.2.blocks.5.norm1.bias\n",
      "stages.2.blocks.5.norm1.bias\n",
      "Module\n",
      "stages.2.blocks.5.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.5.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.2.blocks.5.attn.w_msa.relative_position_index\n",
      "stages.2.blocks.5.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.2.blocks.5.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.5.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.2.blocks.5.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.5.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.2.blocks.5.attn.w_msa.proj.weight\n",
      "stages.2.blocks.5.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.2.blocks.5.attn.w_msa.proj.bias\n",
      "stages.2.blocks.5.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.2.blocks.5.norm2.weight\n",
      "stages.2.blocks.5.norm2.weight\n",
      "Module\n",
      "stages.2.blocks.5.norm2.bias\n",
      "stages.2.blocks.5.norm2.bias\n",
      "Module\n",
      "stages.2.blocks.5.ffn.layers.0.0.weight\n",
      "stages.2.blocks.5.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.2.blocks.5.ffn.layers.0.0.bias\n",
      "stages.2.blocks.5.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.2.blocks.5.ffn.layers.1.weight\n",
      "stages.2.blocks.5.ffn.layers.1.weight\n",
      "Module\n",
      "stages.2.blocks.5.ffn.layers.1.bias\n",
      "stages.2.blocks.5.ffn.layers.1.bias\n",
      "Module\n",
      "stages.2.downsample.norm.weight\n",
      "stages.2.downsample.norm.weight\n",
      "Module\n",
      "stages.2.downsample.norm.bias\n",
      "stages.2.downsample.norm.bias\n",
      "Module\n",
      "stages.2.downsample.reduction.weight\n",
      "stages.2.downsample.reduction.weight\n",
      "Module\n",
      "stages.3.blocks.0.norm1.weight\n",
      "stages.3.blocks.0.norm1.weight\n",
      "Module\n",
      "stages.3.blocks.0.norm1.bias\n",
      "stages.3.blocks.0.norm1.bias\n",
      "Module\n",
      "stages.3.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "stages.3.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.3.blocks.0.attn.w_msa.relative_position_index\n",
      "stages.3.blocks.0.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.3.blocks.0.attn.w_msa.qkv.weight\n",
      "stages.3.blocks.0.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.3.blocks.0.attn.w_msa.qkv.bias\n",
      "stages.3.blocks.0.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.3.blocks.0.attn.w_msa.proj.weight\n",
      "stages.3.blocks.0.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.3.blocks.0.attn.w_msa.proj.bias\n",
      "stages.3.blocks.0.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.3.blocks.0.norm2.weight\n",
      "stages.3.blocks.0.norm2.weight\n",
      "Module\n",
      "stages.3.blocks.0.norm2.bias\n",
      "stages.3.blocks.0.norm2.bias\n",
      "Module\n",
      "stages.3.blocks.0.ffn.experts.0.layers.0.0.weight\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "Module\n",
      "stages.3.blocks.1.norm1.weight\n",
      "stages.3.blocks.1.norm1.weight\n",
      "Module\n",
      "stages.3.blocks.1.norm1.bias\n",
      "stages.3.blocks.1.norm1.bias\n",
      "Module\n",
      "stages.3.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "stages.3.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "Module\n",
      "stages.3.blocks.1.attn.w_msa.relative_position_index\n",
      "stages.3.blocks.1.attn.w_msa.relative_position_index\n",
      "Module\n",
      "stages.3.blocks.1.attn.w_msa.qkv.weight\n",
      "stages.3.blocks.1.attn.w_msa.qkv.weight\n",
      "Module\n",
      "stages.3.blocks.1.attn.w_msa.qkv.bias\n",
      "stages.3.blocks.1.attn.w_msa.qkv.bias\n",
      "Module\n",
      "stages.3.blocks.1.attn.w_msa.proj.weight\n",
      "stages.3.blocks.1.attn.w_msa.proj.weight\n",
      "Module\n",
      "stages.3.blocks.1.attn.w_msa.proj.bias\n",
      "stages.3.blocks.1.attn.w_msa.proj.bias\n",
      "Module\n",
      "stages.3.blocks.1.norm2.weight\n",
      "stages.3.blocks.1.norm2.weight\n",
      "Module\n",
      "stages.3.blocks.1.norm2.bias\n",
      "stages.3.blocks.1.norm2.bias\n",
      "Module\n",
      "stages.3.blocks.1.ffn.layers.0.0.weight\n",
      "stages.3.blocks.1.ffn.layers.0.0.weight\n",
      "Module\n",
      "stages.3.blocks.1.ffn.layers.0.0.bias\n",
      "stages.3.blocks.1.ffn.layers.0.0.bias\n",
      "Module\n",
      "stages.3.blocks.1.ffn.layers.1.weight\n",
      "stages.3.blocks.1.ffn.layers.1.weight\n",
      "Module\n",
      "stages.3.blocks.1.ffn.layers.1.bias\n",
      "stages.3.blocks.1.ffn.layers.1.bias\n",
      "Module\n",
      "norm0.weight\n",
      "norm0.weight\n",
      "Module\n",
      "norm0.bias\n",
      "norm0.bias\n",
      "Module\n",
      "norm1.weight\n",
      "norm1.weight\n",
      "Module\n",
      "norm1.bias\n",
      "norm1.bias\n",
      "Module\n",
      "norm2.weight\n",
      "norm2.weight\n",
      "Module\n",
      "norm2.bias\n",
      "norm2.bias\n",
      "Module\n",
      "norm3.weight\n",
      "norm3.weight\n",
      "Module\n",
      "norm3.bias\n",
      "norm3.bias\n"
     ]
    }
   ],
   "source": [
    "num_experts = 2\n",
    "j = 1\n",
    "ckp_it = iter(ckp_state_dict)\n",
    "model_it = iter(backbone_state_dict)\n",
    "experts_it = iter(experts)\n",
    "l = len([key for key in ckp_state_dict])\n",
    "it = 0\n",
    "stop_it = False\n",
    "while not stop_it:\n",
    "    model_key = next(model_it)\n",
    "    print(\"Module\")\n",
    "    print(model_key)\n",
    "    if 'gate' in model_key:\n",
    "        continue\n",
    "   \n",
    "    if 'expert' in model_key:\n",
    "        ws = []\n",
    "        for i in range(4):\n",
    "            key = next(ckp_it)\n",
    "            w = ckp_state_dict[key]\n",
    "            ws.append(w)\n",
    "            it+=1\n",
    "            if it==l:\n",
    "                stop_it = True\n",
    "                break\n",
    "        i = 0\n",
    "        for _ in range(4*num_experts):\n",
    "            shape = backbone_state_dict[model_key].shape\n",
    "            # shape = [shape[0], shape[1]] if len(shape)==2 else [shape[0]]\n",
    "            print(shape)\n",
    "            print(ws[i%4].shape)\n",
    "            backbone_state_dict[model_key] = ws[i%4].clone()\n",
    "            i+=1\n",
    "            model_key = next(model_it)  \n",
    "        if 'gate' in model_key:\n",
    "            model_key = next(model_it)\n",
    "            continue\n",
    "    ckp_key = next(ckp_it)\n",
    "    print(ckp_key)\n",
    "    it+=1\n",
    "    if it==l:\n",
    "            stop_it = True\n",
    "            break\n",
    "    backbone_state_dict[model_key] = ckp_state_dict[ckp_key].clone()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.runner.checkpoint import save_checkpoint\n",
    "checkpoint_out = dict(state_dict=backbone_state_dict)\n",
    "file = 'moe_base_model.pth'\n",
    "save_checkpoint(checkpoint_out, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
